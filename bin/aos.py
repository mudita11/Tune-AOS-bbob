from __future__ import print_function
import numpy as np
from scipy.stats import rankdata
import math
from scipy.spatial import distance
import sys

from abc import ABC,abstractmethod

# MANUEL: where is Rec_PM and the other AOS methods you implemented for the PPSN paper?
# MUDITA: Rec-PM and other are particular combination of these compenents with their default settings. They have independent folders each for an AOS (total 8 AOS).
# MANUEL: All methods should be here. If they are build with a particular combination, then create a class that instantiates that particular combination.

def debug_print(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)
    
# MANUEL: What is the difference between AOS and unknown AOS?
# MUDITA: I am referring to a combination of these components as Unknown AOS if that combination is not considered in literature.
# MANUEL: I don't understand. What do you use AOS for?
# MUDITA: I think more appropriate name for AOS class is AOS_Update because this class is basically updating components of AOS. Its not an AOS method.

class AOS_Update(object):
    def __init__(self, popsize, F1, F, u, X, f_min, x_min, best_so_far, best_so_far1, n_ops):
        self.popsize = int(popsize)
        self.F1 = np.array(F1)
        self.F = np.array(F)
        self.u = u
        self.X = X
        self.f_min = f_min
        self.x_min = x_min
        self.best_so_far = best_so_far
        self.best_so_far1 = best_so_far1
        self.n_ops = n_ops
        self.max_window_size = 10

        # MANUEL: What is opu?
        # MUDITA: opu represents (op)erator that produced offspring (u).
        self.opu = np.full(self.popsize, 4)
        #[4 for i in range(int(popsize))]; self.opu = np.array(self.opu)
        self.old_opu = self.opu.copy()
        self.number_metric = 7
        self.window = np.full((self.max_window_size, self.number_metric), np.inf)
        self.window[:, 0].fill(-1)
        #self.window = [[np.inf for j in range(self.number_metric)] for i in range(self.max_window_size)]; self.window = np.array(self.window); self.window[:, 0].fill(-1)

        # MANUEL: What are these?
        # MUDITA: gen_window stores the offspring metric data for each offspring when offspring is better than parent. It stores -1 otherwise for that offspring. Its a list. Its structre is as follows: [[[second_dim], [second_dim], [second_dim]], [[],[],[]], ...]. Second_dim has data for each offspring. Three second dim represents population size as 3, contained in third_dim. Third_dim represents a generation. Thus, this [[],[],[]] has data of all offsprings in a generation.
        self.gen_window = [] # print("Inside AOS", type(self.gen_window), self.gen_window)
        #self.total_success = []
        #self.total_unsuccess = []
    
##################################################Offspring Metric definitions##################################################################
    def OM_Update(self):
        """If offspring improves over parent, Offsping Metric (OM) stores (1)fitness of offspring (2)fitness improvemnt from parent to offsping (3)fitness improvemnt from best parent to offsping (4)fitness improvemnt from best so far to offsping (5)fitness improvemnt from median parent fitness to offsping (6)relative fitness improvemnt """
        third_dim = []
        F_min = np.min(self.F)
        F_median = np.median(self.F)
        F_absdiff = np.fabs(self.F1 - self.F)
        for i in range(self.popsize):
            second_dim = np.full(7, -1.0)
            if self.F1[i] <= self.F[i]:
                second_dim[0] = self.opu[i]
                second_dim[1] = np.exp(-self.F1[i])
                second_dim[2] = self.F[i] - self.F1[i]
                if self.F1[i] <= F_min:
                    second_dim[3] = F_min - self.F1[i]
                
                if self.F1[i] <= self.best_so_far:
                    second_dim[4] = self.best_so_far - self.F1[i]
            
                if self.F1[i] <= F_median:
                        second_dim[5] = F_median - self.F1[i]
                
                second_dim[6] = (self.best_so_far / (self.F1[i] + 0.001)) * F_absdiff[i]
            
                # MUDITA: Loop for filling the window as improved offsping are generated
                # MANUEL: What is window? You sometimes index it like a list window[][] and other times like a matrix [, ]
                if np.any(self.window[:, 1] == np.inf):
                    for value in range(self.max_window_size - 1, -1, -1):
                        if self.window[value][0] == -1:
                            self.window[value] = second_dim
                            #print(self.window)
                            break
                else:
                    # MANUEL: What is this code doing? It is not obvious.
                    # MUDITA: When a new sample comes in (second dimension), it finds from the last index a sample to replace that was generated by the same operator application. Once it gets the index of the sample to replace (nn), it removes it and shifts all samples one index below to put that new sample at top of window (index = 0). In case it could not find any existing application of that operator in the window, it replaces the worst candidate.
                    for nn in range(self.max_window_size-1,-1,-1):
                        if self.window[nn][0] == self.opu[i] or nn == 0:
                            for nn1 in range(nn, 0, -1):
                                self.window[nn1] = self.window[nn1-1]
                            self.window[0] = second_dim
                            break
#                       elif nn==0 and self.window[nn][0] != self.opu[i]:
                            # MANUEL: What is window? You sometimes index it like a list window[][] and other times like a matrix [, ].
#                           if self.F1[i] < np.max(self.window[: ,1]):
#                               self.window[np.argmax(self.window[:,1])] = second_dim
            third_dim.append(second_dim)

            # second_dim = np.zeros(7);
            # if self.F1[i] <= self.F[i]:
            #     # success[self.opu[i]] += 1
            #     second_dim[0] = self.opu[i]
            #     second_dim[1] = np.exp(-self.F1[i])
            #     second_dim[2] = self.F[i] - self.F1[i]
            #     if self.F1[i] <= np.min(self.F):
            #         second_dim[3] = np.min(self.F) - self.F1[i]
            #     else:
            #         second_dim[3] = -1
            #     if self.F1[i] <= self.best_so_far:
            #         second_dim[4] = self.best_so_far - self.F1[i]
            #     else:
            #         second_dim[4] = -1
            #     if self.F1[i] <= np.median(self.F):
            #         second_dim[5] = np.median(self.F) - self.F1[i]
            #     else:
            #         second_dim[5] = -1
            #     second_dim[6] = (self.best_so_far / (self.F1[i]+0.001)) * math.fabs(self.F1[i] - self.F[i])
                
            #     if np.any(self.window[:, 1] == np.inf):
            #         for value in range(self.window_size-1,-1,-1):
            #             if self.window[value][0] == -1:
            #                 self.window[value] = second_dim
            #                 #print(self.window)
            #                 break
            #     else:
            #         for nn in range(self.window_size-1,-1,-1):
            #             if self.window[nn][0] == self.opu[i]:
            #                 for nn1 in range(nn, 0, -1):
            #                     self.window[nn1] = self.window[nn1-1]
            #                 self.window[0] = second_dim
            #                 break
            #             elif nn==0 and self.window[nn][0] != self.opu[i]:
            #                 if self.F1[i] < np.max(self.window[: ,1]):
            #                     self.window[np.argmax(self.window[:,1])] = second_dim
            #     #self.X[i][:] = self.u[i][:]
            #     #self.F[i] = self.F1[i]; print("aos", self.F)
            #     third_dim.append(second_dim);# print("r, rule: ",r, rule)
            # else:
            #     # unsuccess[self.opu[i]] += 1
            #     second_dim = [-1 for i in range(7)]
            #     second_dim[0] = self.opu[i]
            #     third_dim.append(second_dim)
        
        #print("Window in AOS update ", self.window, type(self.window), np.shape(self.window))
        self.gen_window.append(third_dim)
        #print("gen_window= ",type(self.gen_window), np.shape(self.gen_window), self.gen_window)
        # self.gen_window = np.array(self.gen_window)
        # self.total_success.append(success); self.total_unsuccess.append(unsuccess); #print("Su",success); print("Un",unsuccess); print("TSu",self.total_success); print("TUn",self.total_unsuccess);
        #print("call to reward")
        self.old_reward = self.reward.copy()
        self.old_probability = self.probability.copy()
        self.reward = self.reward_type.calc_reward()
        self.quality = self.quality_type.calc_quality(self.old_reward, self.reward, self.old_probability) #print("call to probability")
        self.probability = self.probability_type.calc_probability(self.quality)
        self.old_opu = self.opu

##################################################Other definitions######################################################################



def count_op(n_ops, window, Off_met):
    '''Return sorted window, number of successful applications of operators and rank'''
    # Gives rank to window[:, Off_met]: largest number will get largest number rank
    rank = rankdata(window[:, Off_met].round(1), method = 'min')
    order = rank.argsort()
    # order gives the index of rank in ascending order. Sort operators and rank in increasing rank.
    # window_op_sorted is the window sorted from lowest Off_met value to highest and only consists of operators (first column in window).
    window_op_sorted = window[order, 0]
    rank = rank[order]
    rank = rank[window_op_sorted >= 0]
    window_op_sorted = window_op_sorted[window_op_sorted >= 0] # print("window_op_sorted = ",window, window_op_sorted, rank, order)
    # counts number of times an operator is present in the window
    N = np.zeros(n_ops)
    # the number of times each operator appears in the sliding window
    op, count = np.unique(window_op_sorted, return_counts=True)
    N[op.astype(np.int32)] = count
    return window_op_sorted, N, rank

# Count the successful number of applications in fix number of generations

#def count_unsuccessful_applications(gen_window):
    #x, y = np.unique(gen_window[:,:,0], return_counts = True)
    #if np.any(x, -1):
        #for i in range(len(x)):
            #if x[i] == -1:
                #return y[i]
    #else:
        #return 0


def TM(n_ops, p):
    # Calculates Transitive Matrix.
    ## Numpy broadcasting.
    tran_matrix = p + p[:, np.newaxis]
    return normalize_matrix(tran_matrix)

# Normalise n_ops dimensional matrix

def normalize_matrix(x):
    return x / np.sum(x, axis=1)[:, None]

def calc_delta_r(decay, W, window_size, ndcg):
    if decay == 0:
        return np.ones(window_size)
    r = np.arange(float(W))
    if ndcg:
        r += 1
        delta_r = ((2 ** (W - r)) - 1) / np.log(1 + r)
    else:
        delta_r = (decay ** r) * (W - r)
    return delta_r

def AUC(operators, rank, op, decay, window_size, ndcg = True):
    """Calculates area under the curve for each operator"""
    assert len(operators) == len(rank)
    W = len(operators)
    delta_r_vector = calc_delta_r(decay, W, window_size, ndcg)
    x, y, area = 0, 0, 0
    r = 0
    while r < W:
        delta_r = delta_r_vector[r]
        # number of rewards equal to reward ranked r given by op
        tiesY = np.count_nonzero(rank[operators == op] == rank[r])
        # number of rewards equal to reward ranked r given by others
        tiesX = np.count_nonzero(rank[operators != op] == rank[r])
        assert tiesY >= 0
        assert tiesX >= 0
        if (tiesX + tiesY) > 0 :
            delta_r = np.sum(delta_r_vector[r : r + tiesX + tiesY]) / (tiesX + tiesY)
            x += tiesX * delta_r
            area += (y * tiesX * delta_r) + (0.5 * delta_r * delta_r * tiesX * tiesY)
            y += tiesY * delta_r
            r += tiesX + tiesY
        elif operators[r] == op:
            y += delta_r
            r += 1
        else:
            x += delta_r
            area += y * delta_r
            r += 1
    return area

def UCB(N, C, reward):
    # Calculates Upper Confidence Bound as a quality
    ucb = reward + C * np.sqrt( 2 * np.log(np.sum(N)) / N)
    ucb[np.isinf(ucb) | np.isnan(ucb)] = 0
    return ucb

#def count_success(gen_window, i, j, off_met):
#    c_s = np.sum((gen_window[j, :, 0] == i) & (gen_window[j, :, off_met] != -1))
#    c_us = np.sum((gen_window[j, :, 0] == i) & (gen_window[j, :, off_met] == -1))
#    return c_s, c_us


'''
def angle_between(p1, p2):
    # arctan2(y, x) computes the clockwise angle  (a value in radians between -pi and pi) between the origin and the point (x, y)
    ang1 = np.arctan2(*p1[::-1]); # print("ang1", np.rad2deg(ang1))
    ang2 = np.arctan2(*p2[::-1]); # print("ang2", np.rad2deg(ang2))
    # second angle is subtracted from the first to get signed clockwise angular difference, that will be between -2pi and 2pi. Thus to get positive angle between 0 and 2pi, take the modulo against 2pi. Finally radians can be optionally converted to degrees using np.rad2deg.
    #print("angle", np.rad2deg((ang1 - ang2) % (2 * np.pi)))
    if (ang1 - ang2) % (2 * np.pi) > np.pi:
        return 2 * np.pi - ((ang1 - ang2) % (2 * np.pi))
    else:
        return (ang1 - ang2) % (2 * np.pi)
'''


##################################################Reward definitions######################################################################

                                                        ##########################Diversity-Quality based#################################

# index: 0 Applicable for fix number of generation
# Parameter(s): max_gen
#def Reward0(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    s_op = np.zeros(n_ops)
#    q_op = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    if max_gen > len(gen_window):
#        max_gen = len(gen_window)
#    for i in range(n_ops):
#        b = []
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
            #print("GW........",gen_window[j])
#            if np.any(gen_window[j, :, 0] == i):
#                b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, Off_met] != -1), Off_met])
        #print(b)
#        if b != []:
#            s_op[i] = np.std(np.hstack(b)); q_op[i] = np.average(np.hstack(b))
#        else:
#            s_op[i] = 0; q_op[i] = 0
        #print("b", b)
    #a.append(b)
    #print("a", s_op, q_op)
#    for i in range(n_ops):
#        for j in range(n_ops):
#            if i != j:
                #print(gen_window[len(gen_window)-1], i, j)
#                if s_op[i] != [] and s_op[j] != []:
                    #print("D, Q:  ",A1, A2, op_Diversity(gen_window, j, Off_met), op_Quality(gen_window, j, Off_met))
#                    if s_op[i] > s_op[j] and q_op[i] > q_op[j]:
#                        reward[i] += 1
    #print(reward)
#        if np.sum(reward) != 0:
#            reward = (reward / np.sum(reward))
#    return reward


# index: 1 Applicable for fix number of generation
# Parameter(s): max_gen
#def Reward1(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    s_op = np.zeros(n_ops)
#    q_op = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    if max_gen > len(gen_window):
#        max_gen = len(gen_window)
#    for i in range(n_ops):
#        b = []
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
            #print("GW........",gen_window[j])
#            if np.any(gen_window[j, :, 0] == i):
#                b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, Off_met] != -1), Off_met])
        #print(b)
#        if b != []:
#            s_op[i] = np.std(np.hstack(b)); q_op[i] = np.average(np.hstack(b))
#        else:
#            s_op[i] = 0; q_op[i] = 0
        #print("b", b)
    #a.append(b)
    #print("a", s_op, q_op)
#    for i in range(n_ops):
#        for j in range(n_ops):
#            if i != j:
                #print(gen_window[len(gen_window)-1], i, j)
#                if s_op[i] != [] and s_op[j] != []:
                    #print("D, Q:  ",A1, A2, op_Diversity(gen_window, j, Off_met), op_Quality(gen_window, j, Off_met))
#                    if s_op[i] < s_op[j] and q_op[i] < q_op[j]:
#                        reward[i] += 1
    #print(reward)
#    if np.sum(reward) != 0:
#        reward = (reward / np.sum(reward))
#    return -reward


# index: 2 Applicable for current generation
# Parameter(s): max_gen
#def Reward2(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9,int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    if max_gen > len(gen_window):
#        max_gen = len(gen_window)
    # Projection on line B with thetha = pi/4
#    B = [1, 1]
#    for i in range(n_ops):
#        b = []
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
#            if np.any(gen_window[len(gen_window)-1, :, 0] == i):
#                b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, Off_met] != -1), Off_met])
#        if b != []:
            # Diversity: np.std(np.hstack(b)) and Quality: np.average(np.hstack(b))
#            reward[i] = 1-distance.cosine([np.std(np.hstack(b)), np.average(np.hstack(b))], B); # print(b, reward[i])
#    reward = reward - np.min(reward)
#    return reward

                                                        ##########################Comparison (Rank) based########################################

# index: 3 Applicable for fix size window
# Parameter(s): window, decay_reward3
#def Reward3(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9,int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    window = window[window[:, Off_met] != -1][:, :];
#    window_op_sorted, N, rank = count_op(n_ops, window, Off_met); # print(window, window_op_sorted, N, rank)
#    for op in range(n_ops):
#        reward[op] = AUC(window_op_sorted, rank, op, decay_reward3)
    # print("Inside reward: ", reward)
#    return reward

# index: 4 Applicable for fix size window (Done!)
# Parameter(s): window, decay_reward4
#def Reward4(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9,int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    window = window[window[:, Off_met] != -1][:, :];
#    window_op_sorted, N, rank = count_op(n_ops, window, Off_met)
#    for i in range(len(window_op_sorted)):
#        value = (decay_reward4 ** rank[i]) * (window_size - rank[i])
        ## MANUEL: If window_op_sorted only contains values from 0 to n_ops, then this loop can be simply:
        # reward[window_op_sorted] += value
        ## Test it!
#        for j in range(n_ops):
#            if window_op_sorted[i] == j:
#                reward[j] += value
#    reward /= np.sum(reward)
#    return reward

########################Success based###########################################

# index: 5 Applicable for fix number of generations
# Parameter(s): max_gen, int_a_reward5, b_reward5, e_reward5
#def Reward5(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    if len(gen_window) < max_gen:
#        max_gen = len(gen_window)
#    for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
#        total_success = np.zeros(n_ops); total_unsuccess = np.zeros(n_ops)
#        for i in range(n_ops):
#            if np.any(gen_window[j, :, 0] == i):
#                total_success[i], total_unsuccess[i] = count_success(popsize, gen_window, i, j, Off_met)
        #print(total_success, total_unsuccess, np.sum(total_success))
#        for i in range(n_ops):
#            if total_success[i] + total_unsuccess[i] != 0:
#                reward[i] += (total_success[i]**int_a_reward5 + b_reward5 * np.sum(total_success)) / (total_success[i] + total_unsuccess[i])
#            else:
#                reward[i] += 0
#    reward += e_reward5
#    return reward

# index: 6 Applicable for last generation
# Parameter(s): None
#def Reward6(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    for i in range(n_ops):
#        total_success = 0; total_unsuccess = 0
#        if np.any(gen_window[len(gen_window)-1, :, 0] == i):
#            total_success, total_unsuccess = count_success(popsize, gen_window, i, len(gen_window)-1, Off_met); #print(total_unsuccess, total_success)
#            reward[i] = (np.array(total_success) / popsize)
#    return reward

                                                        ##########################Weighted offspring based################################

# index: 7 Applicable for fix number of generations
# Parameter(s): max_gen
#def Reward70(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    appl = np.zeros(n_ops)
#    if len(gen_window) < max_gen:
#        max_gen = len(gen_window)
#    for i in range(n_ops):
#        appl = 0
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
#            total_success = 0; total_unsuccess = 0
            #print("first: ",gen_window, np.shape(gen_window), len(gen_window), i, j);print(gen_window[j,0,0])
#            if np.any(gen_window[j, :, 0] == i):
#                total_success, total_unsuccess = count_success(popsize, gen_window, i, j, Off_met)
#                reward[i] += np.sum(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, Off_met] != -1), Off_met])
#                appl += total_success + total_unsuccess
#        if appl != 0:
#            reward[i] = np.array(reward[i]) / (np.array(appl))
#        else:
#            reward[i] = 0
#    return reward


# index: 8 Applicable for fix window size
# Parameter(s): window, a_reward71
#def Reward71(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    window = window[window[:, Off_met] != -1][:, :]; #print("inside ", window)
#    window_op_sorted, N, rank = count_op(n_ops, window, Off_met)
#    for i in range(n_ops):
#        if np.any(window[:,0] == i):
#            reward[i] += np.sum(window[window[:, 0] == i][:, Off_met]); # print(i, reward[i])
#            reward[i] = np.array(reward[i]) / np.array(N[i])
#    if np.max(reward) != 0:
#        reward = reward / np.max(reward)**a_reward71
#    return reward

# index: 9 Applicable for fix number of generations
# Parameter(s): max_gen
#def Reward8(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9,int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops)
#    gen_window = np.array(gen_window)
#    if len(gen_window) < max_gen:
#        max_gen = len(gen_window)
#    for i in range(n_ops):
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
#            total_success = 0; total_unsuccess = 0
            #print("first: ",gen_window, np.shape(gen_window), len(gen_window), i, j);print(gen_window[j,0,0])
#            if np.any(gen_window[j,:,0] == i):
#                total_success, total_unsuccess = count_success(popsize, gen_window, i, j, Off_met)
#                if total_success + total_unsuccess != 0:
#                    reward[i] += np.sum(gen_window[j, np.where(gen_window[j,:,0] == i) and np.where(gen_window[j, :, Off_met] != -1) , Off_met]) / np.array(total_success + total_unsuccess)
                    #print(reward[i], gen_window[j, np.where(gen_window[j,:,0] == i), Off_met], total_success[j][i])
    # reward = reward / np.array(max_gen)
#    return reward



                                                        ##########################Best offspring metric based#############################

# index: 10 Applicable for last two generations
# Parameter(s): c_reward9, int_b_reward9, int_a_reward9
#def Reward9(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops);
#    best_t = np.zeros(n_ops); best_t_1 = np.zeros(n_ops);
#    gen_window = np.array(gen_window);
#    for i in range(n_ops):
#        n_applications = np.zeros(2) # for last 2 generations
        # Calculating best in current generation
#        if np.any(gen_window[len(gen_window)-1, :, 0] == i):
#            total_success, total_unsuccess = count_success(popsize, gen_window, i, len(gen_window)-1, Off_met)
#            n_applications[0] = total_success + total_unsuccess
#            if np.any(gen_window[len(gen_window)-1, :, Off_met] != -1):
#                best_t[i] = np.max(gen_window[len(gen_window)-1, np.where((gen_window[len(gen_window)-1, :, 0] == i) & (gen_window[len(gen_window)-1, :, Off_met] != -1)), Off_met]); # print(i, best_t[i])
        # Calculating best in last generation
#        if len(gen_window)>=2 and np.any(gen_window[len(gen_window)-2,:,0] == i):
#            total_success, total_unsuccess = count_success(popsize, gen_window, i, len(gen_window)-2, Off_met)
#            n_applications[1] = total_success + total_unsuccess
#            if np.any(gen_window[len(gen_window)-2, :, Off_met] != -1):
#                best_t_1[i] = np.max(gen_window[len(gen_window)-2, np.where((gen_window[len(gen_window)-2, :, 0] == i) & (gen_window[len(gen_window)-2, :, Off_met] != -1)), Off_met]); # print(i, best_t_1[i])
#        if best_t_1[i] != 0 and np.fabs(n_applications[0] - n_applications[1]) != 0:
#            reward[i] = c_reward9 * np.fabs(best_t[i] - best_t_1[i]) / (((best_t_1[i])**int_b_reward9) * (np.fabs(n_applications[0] - n_applications[1])**int_a_reward9))
#        elif best_t_1[i] != 0 and np.fabs(n_applications[0] - n_applications[1]) == 0:
#            reward[i] = c_reward9 * np.fabs(best_t[i] - best_t_1[i]) / ((best_t_1[i])**int_b_reward9)
#        elif best_t_1[i] == 0 and np.fabs(n_applications[0] - n_applications[1]) != 0:
#            reward[i] = c_reward9 * np.fabs(best_t[i] - best_t_1[i]) / (np.fabs(n_applications[0] - n_applications[1])**int_a_reward9)
#        else:
#            reward[i] = c_reward9 * np.fabs(best_t[i] - best_t_1[i])
#    return reward

# index: 11 Applicable for fix size window
# Parameter(s): window
#def Reward100(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, c_reward6, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward100, b_reward100, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
    #reward = np.zeros(n_ops)
    #for i in range(n_ops):
        #if np.any(window[:,0] == i):
            #reward[i] = np.sum(window[window[:, 0] == i][:, Off_met]); # print(reward[i])
    #return reward

# index: 11 Applicable for fix number of generations
# Parameter(s): max_gen, b_reward101, int_a_reward101
#def Reward101(popsize, n_ops, window_size, window, gen_window, Off_met, max_gen, decay_reward3, decay_reward4, int_a_reward5, b_reward5, e_reward5, a_reward71, c_reward9, int_b_reward9, int_a_reward9, int_a_reward101, b_reward101, opu, old_opu, total_success, total_unsuccess, old_reward):
#    reward = np.zeros(n_ops); gen_window = np.array(gen_window)
#    if len(gen_window) < max_gen:
#        max_gen = len(gen_window)
#    for i in range(n_ops):
#        gen_best = []
#        for j in range(len(gen_window)-1, len(gen_window)-max_gen-1, -1):
            # print(gen_window)
            # print("first: ", i, j, np.hstack(gen_window[j, np.where((gen_window[j,:,0] == i) & (gen_window[j, :, Off_met] != -1)), Off_met]))
#            if np.any((gen_window[j,:,0] == i) & (gen_window[j, :, Off_met] != -1)):
                # print("inside")
#                gen_best.append(np.max(np.hstack(gen_window[j, np.where((gen_window[j,:,0] == i) & (gen_window[j, :, Off_met] != -1)), Off_met])))
#                reward[i] += np.sum(gen_best); # print(reward[i])
#        if gen_best != []:
#            reward[i] = (1/max_gen) * reward[i]**b_reward101 / np.max(gen_best)**int_a_reward101
#    return reward


##################################################Quality definitions#####################################################################

# Parameter(s): adaptation_rate
#def Quality0(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops); # print(old_quality, reward, old_reward)
#    Q = old_quality + adaptation_rate * (reward - old_quality)
    # Q = Q - np.max(Q)
    # Q = np.exp(Q)
    # Q = Q / np.sum(Q)
#    return Q

# Parameter(s): scaling_factor
#def Quality1(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops)
#    window_op_sorted, N, rank = count_op(n_ops, window, Off_met)
#    Q = upper_confidence_bound (N, scaling_factor, reward); # print(window_op_sorted, N, rank, reward, Q)
#    return Q

# Parameter(s): phi
#def Quality2(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops)
#    Q = np.exp(reward/phi)
#    return Q

#def Quality3(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops)
#    Q = reward
#    return Q

'''def Quality4(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, discount_rate, delta, Off_met, old_probability, old_reward):
    tran_matrix = normalize_matrix(np.random.rand(n_ops, n_ops)); # print(old_probability)
    tran_matrix = TM(n_ops, old_probability)
    Q = np.matmul(np.linalg.pinv(np.array(1 - discount_rate * tran_matrix)), np.array(reward))
    Q = Q - np.max(Q)
    Q = np.exp(Q)
    # Normalisation for all operators
    Q = Q / np.sum(Q)
    return Q'''

'''def Quality5(n_ops, alpha, reward, old_quality, scaling_factor, window, Off_met, old_probability, old_reward):
    Q = np.zeros(n_ops)
    window_op_sorted, N, rank = count_op(n_ops, window, Off_met)
    Q = (reward + scaling_factor(np.sum(reward)))/N
    return Q'''

# Parameter(s): delta
#def Quality4(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops)
#    if np.sum(reward) > 0:
#        Q = delta * (reward /np.sum(reward))  + (1 - delta) * old_quality
#    else:
#        Q = delta * n_ops + (1 - delta) * old_quality
#    return Q

# Parameter(s): c1_quality6, c2_quality6, discount_rate
#def Quality5(n_ops, adaptation_rate, reward, old_quality, phi, window, scaling_factor, c1_quality6, c2_quality6, discount_rate, delta, Off_met, old_probability, old_reward):
#    Q = np.zeros(n_ops)
#    tran_matrix = TM(n_ops, old_probability)
#    tran_matrix = normalize_matrix(np.random.rand(n_ops, n_ops)); # print(old_probability)

#    Q = c1_quality6 * reward + c2_quality6 * old_reward
#    if discount_rate != 0:
#        Q = np.matmul(np.linalg.pinv(np.array(1 - discount_rate * tran_matrix)), np.array(Q))
#        Q = Q - np.max(Q)
#        Q = np.exp(Q)
#        # Normalisation for all operators
#        Q = Q / np.sum(Q)
#    return Q


def angle(vec, theta):
    vec = vec * np.sign(vec[1])
    angle = np.arccos(1 - distance.cosine(vec, np.array([1, 0]))) # In radian
    return angle - np.deg2rad(theta)

##################################################Reward definitions######################################################################


def build_reward(choice, n_ops, rew_args, gen_window, window, off_met, popsize):
    if choice == 0:
        return Pareto_Dominance(n_ops, off_met, gen_window, rew_args["fix_appl"])
    elif choice == 1:
        return Pareto_Rank(n_ops, off_met, gen_window, rew_args["fix_appl"])
    elif choice == 2:
        return Compass_projection(n_ops, off_met, gen_window, rew_args["fix_appl"], rew_args["theta"])
    elif choice == 3:
        return Area_Under_The_Curve(n_ops, off_met, window, rew_args["window_size"], rew_args["decay"])
    elif choice == 4:
        return Sum_of_Rank(n_ops, off_met, window, rew_args["window_size"], rew_args["decay"])
    elif choice == 5:
        return Success_Rate1(n_ops, off_met, gen_window, rew_args["max_gen"], rew_args["succ_lin_quad"], rew_args["frac"], rew_args["noise"])
    elif choice == 6:
        return Success_Rate2(n_ops, off_met, gen_window, popsize)
    elif choice == 7:
        return Success_sum(n_ops, off_met, gen_window, rew_args["max_gen"])
    elif choice == 8:
        return Normalised_success_sum_window(n_ops, off_met, window, rew_args["window_size"], rew_args["normal_factor"])
    elif choice == 9:
        return Normalised_success_sum_gen(n_ops, off_met, gen_window, rew_args["max_gen"])
    elif choice == 10:
        return Best2gen(n_ops, off_met, gen_window, rew_args["scaling_constant"], rew_args["choice2"], rew_args["choice3"])
    elif choice == 11:
        return Normalised_best_sum(n_ops, off_met, gen_window, rew_args["max_gen"], rew_args["intensity"], rew_args["choice4"])
    else:
        raise ValueError("choice {} unknown".format(choice))

class RewardType(ABC):
    def __init__(self, n_ops, off_met, max_gen = None, window_size = None, decay = None, fix_appl = None):
        self.n_ops = n_ops
        # MANUEL: What is this?
        # MUDITA: Offpsing metric in range [1,7] stored in gen_window.
        self.off_met = off_met
        self.max_gen = max_gen
        # MANUEL: So you have window_size here but no window?
        self.window_size = window_size
        self.fix_appl = fix_appl
        self.decay = decay
        self.old_reward = np.zeros(self.n_ops)
    
    def check_reward(self, reward):
        # Nothing to check
        self.old_reward[:] = reward
        return reward

    def generation_window(self):
        return self.gen_window
    
    # Please describe this function! Give it a better name!
    def truncate_window(self):
        return self.window[(self.window[:, self.off_met] != -1) & (self.window[:, self.off_met] != np.inf)][:int(self.window_size)]
    
    def count_op_in_window(self, window):
        return count_op(self.n_ops, window, self.off_met) # print(window, window_op_sorted, N, rank)
    
    def count_total_succ_unsucc(n_ops, gen_window, j, off_met):
        # Counts the number of success and failure for each operator in generation 'j'
        total_success = np.zeros(n_ops)
        total_unsuccess = np.zeros(n_ops)
        for i in range(n_ops):
            if np.any(gen_window[j, :, 0] == i):
                # total_success[i], total_unsuccess[i] = count_success(gen_window, i, j, off_met)
                total_success[i] = np.sum((gen_window[j, :, 0] == i) & (gen_window[j, :, off_met] != -1))
                total_unsuccess[i] = np.sum((gen_window[j, :, 0] == i) & (gen_window[j, :, off_met] == -1))
        return total_success, total_unsuccess

    @abstractmethod
    def calc_reward(self):
        pass

# MANUEL: These should have more descriptive names and a doctstring documenting
# where they come from (references) and what they do.
class Pareto_Dominance(RewardType):
    """
Jorge Maturana, Fr ́ed ́eric Lardeux, and Frederic Saubion. “Autonomousoperator management for evolutionary algorithms”. In:Journal of Heuris-tics16.6 (2010).https://link.springer.com/content/pdf/10.1007/s10732-010-9125-3.pdf, pp. 881–909.
"""

    def __init__(self, n_ops, off_met, gen_window, fix_appl = 20):
        super().__init__(n_ops, off_met, fix_appl = fix_appl)
        # MANUEL: There are other objects with self.gen_window defined!
        self.gen_window = gen_window
        debug_print("\n {} : fix_appl = {}".format(type(self).__name__, self.fix_appl))
    
    def calc_reward(self):
        # MANUEL: This function and the one for Pareto_Rank are almost identical! What's the difference?
        # MUDITA: Pareto dominance returns the number of operators dominated by an operator whereas Pareto rank gives the number of operators an operator is dominated by. Is there a library to calculate these values?
        reward = np.zeros(self.n_ops)
        s_op = np.zeros(self.n_ops)
        q_op = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        # MANUEL: Why does it need to be converted to an array here?
        # MUDITA: In AOS_Update class, it is list because append works on list not array. So here I converted list to array.
        gen_window = np.array(gen_window)
        #print(type(gen_window), np.shape(gen_window), gen_window)
        for i in range(self.n_ops):
            b = []
            count = 0
            for j in range(len(gen_window)-1, 0, -1):
                if np.any(gen_window[j, :, 0] == i):
                    count += 1
                    # MANUEL: What is this doing?
                    # MUDITA: List b collects all the offspring metric data produced by operator i when offspring metric value is not -1.
                    b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, self.off_met] != -1), self.off_met])
                    if count == self.fix_appl:
                        break
            if b != []:
                s_op[i] = np.std(np.hstack(b))
                q_op[i] = np.mean(np.hstack(b))
        #print("b", b)
        #a.append(b)
        #print("a", s_op, q_op)
        for i in range(self.n_ops):
            for j in range(self.n_ops):
                if i != j:
                    #print(gen_window[len(gen_window)-1], i, j)
                    # MANUEL: This comparison is very strange! What are you trying to do?
                    if s_op[i] != [] and s_op[j] != []:
                        #print("D, Q:  ",A1, A2, op_Diversity(gen_window, j, Off_met), op_Quality(gen_window, j, Off_met))
                        if s_op[i] > s_op[j] and q_op[i] > q_op[j]:
                            reward[i] += 1
        if np.sum(reward) != 0:
            reward /= np.sum(reward)
        return super().check_reward(reward)


class Pareto_Rank(RewardType):
    """
Jorge Maturana, Fr ́ed ́eric Lardeux, and Frederic Saubion. “Autonomousoperator management for evolutionary algorithms”. In:Journal of Heuris-tics16.6 (2010).https://link.springer.com/content/pdf/10.1007/s10732-010-9125-3.pdf, pp. 881–909.
"""
    def __init__(self, n_ops, off_met, gen_window, fix_appl = 20):
        super().__init__(n_ops, off_met, fix_appl = fix_appl)
        self.gen_window = gen_window
        debug_print("\n {} : fix_appl = {}".format(type(self).__name__, self.fix_appl))

    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        s_op = np.zeros(self.n_ops)
        q_op = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        for i in range(self.n_ops):
            b = []
            count = 0
            for j in range(len(gen_window)-1, 0, -1):
                if np.any(gen_window[j, :, 0] == i):
                    count += 1
                    b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, self.off_met] != -1), self.off_met])
                    if count == self.fix_appl:
                        break
            if b != []:
                s_op[i] = np.std(np.hstack(b)); q_op[i] = np.average(np.hstack(b))
        for i in range(self.n_ops):
            for j in range(self.n_ops):
                if i != j:
                    #print(gen_window[len(gen_window)-1], i, j)
                    if s_op[i] != [] and s_op[j] != []:
                        #print("D, Q:  ",A1, A2, op_Diversity(gen_window, j, Off_met), op_Quality(gen_window, j, Off_met))
                        if s_op[i] < s_op[j] and q_op[i] < q_op[j]:
                            reward[i] += 1
        #print(reward)
        if np.sum(reward) != 0:
            reward /= np.sum(reward)
        return super().check_reward(-reward)


class Compass_projection(RewardType):
    """
        Jorge Maturana and Fr ́ed ́eric Saubion. “A compass to guide genetic al-gorithms”. In:International Conference on Parallel Problem Solving fromNature.http://www.info.univ-angers.fr/pub/maturana/files/MaturanaSaubion-Compass-PPSNX.pdf. Springer. 2008, pp. 256–265.
        """
    def __init__(self, n_ops, off_met, gen_window, fix_appl = 100, theta = 45):
        super().__init__(n_ops, off_met, fix_appl = fix_appl)
        self.gen_window = gen_window
        self.theta = theta
        debug_print("\n {} : fix_appl = {}".format(type(self).__name__, self.fix_appl, self.theta))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        # MANUEL: This should be an array alreadY?
        # MUDITA: No, in AOS_Update class, it is list because append works on list not array. So here I converted list to array.
        gen_window = np.array(gen_window)
        gen_window_len = len(gen_window)
        # Projection on line B with thetha = pi/4
#        B = [1, 1]
        for i in range(self.n_ops):
            b = []
            count = 0
            for j in range(gen_window_len-1, 0, -1):
                if np.any(gen_window[gen_window_len-1, :, 0] == i):
                    count += 1
                    b.append(gen_window[j, np.where(gen_window[j, :, 0] == i) and np.where(gen_window[j, :, self.off_met] != -1), self.off_met])
                    if count == self.fix_appl:
                        break
            if b != []:
                # Diversity: np.std(np.hstack(b)) and Quality: np.average(np.hstack(b))
                SD = np.std(np.hstack(b)); AVG = np.mean(np.hstack(b))
                #reward[i] = (np.sqrt(np.std(np.hstack(b))**2 + np.average(np.hstack(b))**2)) * np.cosine(np.fabs(np.angle(SD + AVGj) - np.deg2rad(self.theta)))
                # Where does this formula come from?
                reward[i] = (np.sqrt(SD**2 + AVG**2)) * angle(np.array([SD, AVG]), self.theta)
        reward = reward - np.min(reward)
        print("reward",reward)
        return super().check_reward(reward)

class Area_Under_The_Curve(RewardType):
    """
Alvaro Fialho, Marc Schoenauer, and Mich`ele Sebag. “Toward comparison-based adaptive operator selection”. In:Proceedings of the 12th annual con-ference on Genetic and evolutionary computation.https://hal.inria.fr/file/index/docid/471264/filename/banditGECCO10.pdf. ACM.2010, pp. 767–774
"""
    def __init__(self, n_ops, off_met, window, window_size = 50, decay = 0.4):
        super().__init__(n_ops, off_met, window_size = window_size, decay = decay)
        # MANUEL: This window is not the same object as the one in AOS!
        self.window = window
        #print("Window in AUC init ", self.window, type(self.window), np.shape(self.window))
        debug_print("\n {} : window_size = {}, decay = {}".format(type(self).__name__, self.window_size, self.decay))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        #print("Window in AUC calc ", self.window, type(self.window), np.shape(self.window))
        window_op_sorted, N, rank = super().count_op_in_window(super().truncate_window())
        for op in range(self.n_ops):
            reward[op] = AUC(window_op_sorted, rank, op, self.window_size, self.decay)
            # print("Inside reward: ", reward)
        return super().check_reward(reward)

class Sum_of_Rank(RewardType):
    """
Alvaro Fialho, Marc Schoenauer, and Mich`ele Sebag. “Toward comparison-based adaptive operator selection”. In:Proceedings of the 12th annual con-ference on Genetic and evolutionary computation.https://hal.inria.fr/file/index/docid/471264/filename/banditGECCO10.pdf. ACM.2010, pp. 767–774
"""
    def __init__(self, n_ops, off_met, window, window_size = 50, decay = 0.4):
        super().__init__(n_ops, off_met, window_size = window_size, decay = decay)
        self.window = window
        # MANUEL: Isn't it an array already? What is it?
        self.window = np.array(self.window)
        debug_print("\n {} : window_size = {}, decay = {}".format(type(self).__name__, self.window_size, self.decay))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        window_op_sorted, N, rank = super().count_op_in_window(super().truncate_window())
        assert len(window_op_sorted) == len(rank)
        # MANUEL: where does this formula come from?
        # MUDITA: It is from Alvaro's thesis.
        # MANUEL: Please double-check this code
        # MUDITA: Its correct. Please check: https://tel.archives-ouvertes.fr/tel-00578431/document (pg. 79).
        value = (self.decay ** rank) * (self.window_size - rank)
        # MUDITA: Not working as expected.
        reward[window_op_sorted] += value
        # for i in range(len(window_op_sorted)):
        #     ## MANUEL: If window_op_sorted only contains values from 0 to n_ops, then this loop can be simply:
        #     # reward[window_op_sorted] += value
        #     ## Test it!
        #     for j in range(self.n_ops):
        #         if window_op_sorted[i] == j:
        #             reward[j] += value[i]
        if np.sum(reward) != 0:
            reward /= np.sum(reward)
        return super().check_reward(reward)

# MANUEL: Create a new class SuccessRateType. That should contain the gen_window and this common function:
# MUDITA: I have added this function in abstract class RewardType.
#def count_total_succ_unsucc(n_ops, gen_window, j, off_met):
#    total_success = np.zeros(n_ops)
#    total_unsuccess = np.zeros(n_ops)
#    for i in range(n_ops):
        # MANUEL: What is gen_window? Is it a matrix, a list, a cube?
        # MUDITA: It is a 3-D list.
#        if np.any(gen_window[j, :, 0] == i):
#            total_success[i], total_unsuccess[i] = count_success(gen_window, i, j, off_met)
#    return total_success, total_unsuccess


class Success_Rate1(RewardType):
    """ 

A Kai Qin, Vicky Ling Huang, and Ponnuthurai N Suganthan. “Differ-
ential evolution algorithm with strategy adaptation for global numeri-
cal optimization”. In: IEEE transactions on Evolutionary Computation
13.2 (2009). https://www.researchgate.net/profile/Ponnuthurai_
Suganthan/publication/224330344_Differential_Evolution_Algorithm_
With_Strategy_Adaptation_for_Global_Numerical_Optimization/
links/0c960525d39935a20c000000.pdf, pp. 398–417.

With noise == 0, we get

Bryant A Julstrom. “Adaptive operator probabilities in a genetic algo-
rithm that applies three operators”. In: Proceedings of the 1997 ACM
symposium on Applied computing. http://delivery.acm.org/10.1145/
340000/331746/p233-julstrom.pdf?ip=144.32.48.138&id=331746&
acc=ACTIVE%20SERVICE&key=BF07A2EE685417C5%2E26BE4091F5AC6C0A%
2E4D4702B0C3E38B35 % 2E4D4702B0C3E38B35 & _ _ acm _ _ = 1540905461 _
4567820ac9495f6bfbb8462d1c4244a3. ACM. 1997, pp. 233–238.

"""
    
    def __init__(self, n_ops, off_met, gen_window, max_gen = 10, succ_lin_quad = 1, frac = 0.01, noise = 0.0):
        super().__init__(n_ops, off_met, max_gen = max_gen)
        self.gen_window = gen_window
        self.succ_lin_quad = succ_lin_quad
        self.frac = frac
        self.noise = noise
        debug_print("\n {} : max_gen = {}, succ_lin_quad = {}, frac = {}, noise = {}".format(type(self).__name__, self.max_gen, self.succ_lin_quad, self.frac, self.noise))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        # MANUEL: Should be an array already?
        # MUDITA: No, in AOS_Update class, it is list because append works on list not array. So here I converted list to array.
        gen_window = np.array(gen_window)
        gen_window_len = len(gen_window)
        if gen_window_len < self.max_gen:
            self.max_gen = gen_window_len
        for j in range(gen_window_len-1, gen_window_len-self.max_gen-1, -1):
            total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window, j, off_met)
            total = total_success + total_unsuccess
            # Avoid division by zero. If total == 0, then total_success is zero.
            total[total == 0] = 1
            # MANUEL Where does this formula come from?
            # MUDITA: Its part of proposed framework. Combination from 5 papers
            reward += (total_success ** self.succ_lin_quad + self.frac * np.sum(total_success)) / total
            # MANUEL: The loop below replaced by the line above
            # MUDITA: This is part of the framework.
            # for i in range(self.n_ops):
            #     if total_success[i] + total_unsuccess[i] != 0:
            #         reward[i] += (total_success[i]**self.succ_lin_quad + self.frac * np.sum(total_success)) / total[i]
                # MANUEL: Summing zero is useless
                # else:
                #     reward[i] += 0
        reward += self.noise
        return super().check_reward(reward)


class Success_Rate2(RewardType):
    """
 Mudita  Sharma,  Manuel  L ́opez-Ib ́a ̃nez,  and  Dimitar  Kazakov.  “Perfor-mance Assessment of Recursive Probability Matching for Adaptive Oper-ator Selection in Differential Evolution”. In:International Conference onParallel Problem Solving from Nature.http://eprints.whiterose.ac.uk/135483/1/paper_66_1_.pdf. Springer. 2018, pp. 321–333.
 """
    def __init__(self, n_ops, off_met, gen_window, popsize):
        super().__init__(n_ops, off_met)
        self.gen_window = gen_window
        self.popsize = popsize
    
    def calc_reward(self):
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window, len(gen_window) - 1, off_met)
        # for i in range(self.n_ops):
        #     if np.any(self.gen_window[len(self.gen_window)-1, :, 0] == i):
        #         total_success, total_unsuccess = count_success(self.gen_window, i, len(self.gen_window)-1, self.off_met); #print(total_unsuccess, total_success)
        #         reward[i] = np.array(total_success) / self.popsize
        reward = total_success / self.popsize
        return super().check_reward(reward)

class Success_sum(RewardType):
    """
 Christian  Igel  and  Martin  Kreutz.  “Operator  adaptation  in  evolution-ary  computation  and  its  application  to  structure  optimization  of  neu-ral  networks”.  In:Neurocomputing55.1-2  (2003).https : / / ac . els -cdn.com/S0925231202006288/1-s2.0-S0925231202006288-main.pdf?_tid=c6274e78-02dc-4bf6-8d92-573ce0bed4c4&acdnat=1540907096_d0cc1e2b4ca56a49587b4d55e1008a84, pp. 347–361.
 """
    def __init__(self, n_ops, off_met, gen_window, max_gen = 4):
        super().__init__(n_ops, off_met, max_gen = max_gen)
        self.gen_window = gen_window
        debug_print("\n {} : max_gen = {}".format(type(self).__name__, self.max_gen))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        total_appl = np.zeros(self.n_ops)
        gen_window_len = len(gen_window)
        if gen_window_len < self.max_gen:
            self.max_gen = gen_window_len
        # MANUEL: Why is this looping first over n_ops, then over the window? It is so different from SuccessRate1????
        # MUDITA: I have changed the order.
        # MANUEL: Can this use count_total_succ_unsucc()?
        # MUDITA: Yes, used!
#        for i in range(self.n_ops):
#            appl = 0
#            for j in range(len(self.gen_window)-1, gen_window_len-self.max_gen-1, -1):
#                total_success = 0; total_unsuccess = 0
#                if np.any(self.gen_window[j, :, 0] == i):
#                    total_success, total_unsuccess = count_success(popsize, self.gen_window, i, j, self.off_met)
#                    reward[i] += np.sum(self.gen_window[j, np.where(self.gen_window[j, :, 0] == i) & np.where(self.gen_window[j, :, self.off_met] != -1), self.off_met])
#                    appl += total_success + total_unsuccess
#            if appl != 0:
#                reward[i] = reward[i] / appl

        for j in range(gen_window_len-1, gen_window_len-self.max_gen-1, -1):
            total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window, j, off_met)
            total_appl = [sum(x) for x in zip(total_appl, [sum(x) for x in zip(total_success, total_unsuccess)])]
            for i in range(n_ops):
                if np.any(gen_window[j, :, 0] == i):
                    reward[i] += np.sum(gen_window[j, np.where(gen_window[j, :, 0] == i) & np.where(gen_window[j, :, self.off_met] != -1), self.off_met])
        total_appl[total_appl == 0] = 1
        reward = reward/total_appl
        return super().check_reward(reward)

class Normalised_success_sum_window(RewardType):
    """
Alvaro Fialho, Marc Schoenauer, and Mich`ele Sebag. “Analysis of adaptiveoperator selection techniques on the royal road and long k-path problems”.In:Proceedings of the 11th Annual conference on Genetic and evolutionarycomputation.https://hal.archives-ouvertes.fr/docs/00/37/74/49/PDF/banditGECCO09.pdf. ACM. 2009, pp. 779–786.
"""
    def __init__(self, n_ops, off_met, window, window_size = 50, normal_factor = 0.1):
        super().__init__(n_ops, off_met, window_size = window_size)
        self.window = window
        self.normal_factor = normal_factor
        debug_print("\n {} : window_size = {}, normal_factor = {}".format(type(self).__name__, self.window_size, self.normal_factor))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        # Create a local truncated window.
        # window = super().truncate_window()
        window_op_sorted, N, rank = super().count_op_in_window(super().truncate_window())
        for i in range(self.n_ops):
            if np.any(window[:,0] == i):
                reward[i] += np.sum(window[window[:, 0] == i][:, self.off_met]); # print(i, reward[i])
                if N[i]!=0:
                    reward[i] = reward[i] / N[i]
        if np.max(reward) != 0:
            reward /= np.max(reward)**self.normal_factor
        return super().check_reward(reward)

class Normalised_success_sum_gen(RewardType):
    """
Christian Igel and Martin Kreutz. “Using fitness distributions to improvethe evolution of learning structures”. In:Evolutionary Computation, 1999.CEC 99. Proceedings of the 1999 Congress on. Vol. 3.http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.2107&rep=rep1&type=pdf. IEEE. 1999, pp. 1902–1909.
"""
    def __init__(self, n_ops, off_met, gen_window, max_gen = 4):
        super().__init__(n_ops, off_met, max_gen = max_gen)
        self.gen_window = gen_window
        debug_print("\n {} : max_gen = {}".format(type(self).__name__, self.max_gen))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        gen_window_len = len(gen_window)
        if gen_window_len < self.max_gen:
            max_gen = gen_window_len
        for i in range(self.n_ops):
            for j in range(gen_window_len-1, gen_window_len-self.max_gen-1, -1):
                # PLease use count_total_succ_and_unsucc()
                #total_success = 0; total_unsuccess = 0
                total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window, j, off_met)
                total = total_success + total_unsuccess
                if np.any(gen_window[j,:,0] == i):
#                    total_success, total_unsuccess = count_success(self.gen_window, i, j, self.off_met)
                    total[total == 0] = 1
#                    if total_success + total_unsuccess != 0:
                    reward[i] += np.sum(gen_window[j, np.where(gen_window[j,:,0] == i) & np.where(gen_window[j, :, self.off_met] != -1) , self.off_met]) / total
        return super().check_reward(reward)

class Best2gen(RewardType):
    """
 Giorgos Karafotias, Agoston Endre Eiben, and Mark Hoogendoorn. “Genericparameter  control  with  reinforcement  learning”.  In:Proceedings of the2014 Annual Conference on Genetic and Evolutionary Computation.http://www.few.vu.nl/~gks290/papers/GECCO2014-RLControl.pdf. ACM.2014, pp. 1319–1326.
 """
    def __init__(self, n_ops, off_met, gen_window, scaling_constant = 1, choice2 = 0, choice3 = 1):
        super().__init__(n_ops, off_met)
        self.gen_window = gen_window
        self.scaling_constant = scaling_constant
        self.choice2 = choice2
        self.choice3 = choice3
        debug_print("\n {} : scaling constant = {}, choice2 = {}, choice3 = {}".format(type(self).__name__, self.scaling_constant, self.choice2, self.choice3))
    
    def calc_reward(self):
        # Involves calculation of best in previous two generations.
        # MANUEL: Don't use ";" it makes code harder to read!
        reward = np.zeros(self.n_ops)
        best_t = np.zeros(self.n_ops)
        best_t_1 = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        gen_window_len = len(gen_window)
        n_applications = np.zeros(2)
        # MANUEL: Use count_total_succ_unsucc()
        total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window, gen_window_len - 1, off_met)
        n_applications[0] = total_success + total_unsuccess
        if gen_window_len>=2 and np.any(gen_window[gen_window_len-2,:,0] == i):
            total_success, total_unsuccess = super().count_total_succ_unsucc(n_ops, gen_window_len-2, j, off_met)
        n_applications[1] = total_success + total_unsuccess
        n_applications = n_applications[0] - n_applications[1]
        for i in range(self.n_ops):
            # Calculating best in current generation
            if np.any(gen_window[gen_window_len-1, :, self.off_met] != -1):
                best_t[i] = np.max(gen_window[gen_window_len-1, np.where((gen_window[gen_window_len-1, :, 0] == i) & (gen_window[gen_window_len-1, :, self.off_met] != -1)), self.off_met])
            # Calculating best in last generation
            if np.any(gen_window[gen_window_len-2, :, self.off_met] != -1):
                best_t_1[i] = np.max(gen_window[gen_window_len-2, np.where((gen_window[gen_window_len-2, :, 0] == i) & (gen_window[gen_window_len-2, :, self.off_met] != -1)), self.off_met])
        # MANUEL: Use vector operations!
        best_t_1[best_t_1 == 0] = 1
        n_applications[n_applications == 0] = 1
        reward = (self.scaling_constant * np.fabs(best_t - best_t_1)) / (best_t_1**self.choice2) * (np.fabs(n_applications[0] - n_applications[1])**self.choice3)
   
            # MANUEL: Please compare the above with what you wrote below! What is clearer? You need to spend a bit more effort on things and on thinking.
            # MUDITA: Thanks. I will.
            # if best_t_1[i] != 0 and np.fabs(n_applications[0] - n_applications[1]) != 0:
            #     reward[i] = self.scaling_constant * np.fabs(best_t[i] - best_t_1[i]) / (((best_t_1[i])**self.choice2) * (np.fabs(n_applications[0] - n_applications[1])**self.choice3))
            # elif best_t_1[i] != 0 and np.fabs(n_applications[0] - n_applications[1]) == 0:
            #     reward[i] = self.scaling_constant * np.fabs(best_t[i] - best_t_1[i]) / ((best_t_1[i])**self.choice2)
            # elif best_t_1[i] == 0 and np.fabs(n_applications[0] - n_applications[1]) != 0:
            #     reward[i] = self.scaling_constant * np.fabs(best_t[i] - best_t_1[i]) / (np.fabs(n_applications[0] - n_applications[1])**self.choice3)
            # else:
            #     reward[i] = self.scaling_constant * np.fabs(best_t[i] - best_t_1[i])
        return super().check_reward(reward)

class Normalised_best_sum(RewardType):
    """
Alvaro Fialho, Marc Schoenauer, and Mich`ele Sebag. “Analysis of adaptiveoperator selection techniques on the royal road and long k-path problems”.In:Proceedings of the 11th Annual conference on Genetic and evolutionarycomputation.https://hal.archives-ouvertes.fr/docs/00/37/74/49/PDF/banditGECCO09.pdf. ACM. 2009, pp. 779–786.
"""
    def __init__(self, n_ops, off_met, gen_window, max_gen = 10, intensity = 0, choice4 = 1):
        super().__init__(n_ops, off_met, max_gen = max_gen)
        self.gen_window = gen_window
        self.intensity = intensity
        self.choice4 = choice4
        debug_print("\n {} : max_gen = {}, intensity = {}, choice4 = {}".format(type(self).__name__, self.max_gen, self.intensity, self.choice4))
    
    def calc_reward(self):
        reward = np.zeros(self.n_ops)
        gen_window = super().generation_window()
        gen_window = np.array(gen_window)
        gen_window_len = len(gen_window)
        if gen_window_len < self.max_gen:
            self.max_gen = gen_window_len
        for i in range(self.n_ops):
            gen_best = []
            for j in range(gen_window_len-1, gen_window_len-self.max_gen-1, -1):
                # MANUEL: Use count_total_succ_unsucc()
                # MUDITA: We donot use this information (number of applications of an operator) here.
                if np.any((gen_window[j,:,0] == i) & (gen_window[j, :, self.off_met] != -1)):
                    gen_best.append(np.max(np.hstack(gen_window[j, np.where((gen_window[j,:,0] == i) & (gen_window[j, :, self.off_met] != -1)), self.off_met])))
                    reward[i] += np.sum(gen_best)
        reward = (1/max_gen) * reward**self.intensity / np.max(gen_best)**self.choice4
        return super().check_reward(reward)


##################################################Quality definitions######################################################################

def build_quality(choice, n_ops, qual_args, window, off_met):
    if choice == 0:
        return Weighted_sum(n_ops, qual_args["adaptation_rate"])
    elif choice == 1:
        return Upper_confidence_bound(n_ops, off_met, window, qual_args["scaling_factor"])
    elif choice == 2:
        return Identity(n_ops)
    elif choice == 3:
        return Weighted_normalised_reward(n_ops, qual_args["decay_rate"])
    elif choice == 4:
        return Markov_reward_process(n_ops, qual_args["memory_curr_reward"], qual_args["memory_prev_reward"], qual_args["discount_rate"])
    else:
        raise ValueError("choice {} unknown".format(choice))

class QualityType(ABC):
    def __init__(self, n_ops):
        self.n_ops = n_ops
        self.old_quality = np.zeros(n_ops)
        pass
    
    def check_quality(self, quality):
        assert np.sum(quality) >= 0
        if np.sum(quality) != 0:
            quality /= np.sum(quality)
        assert np.all(quality >= -0.0)
        self.old_quality = quality
        return(quality)
    
    @abstractmethod
    def calc_quality(self, old_reward, reward):
        pass

# MANUEL: These should have more descriptive names and a doctstring documenting
# where they come from (references) and what they do.
class Weighted_sum(QualityType):
    """
 Dirk Thierens. “An adaptive pursuit strategy for allocating operator prob-abilities”.  In:Proceedings of the 7th annual conference on Genetic andevolutionary computation.http://www.cs.bham.ac.uk/~wbl/biblio/gecco2005/docs/p1539.pdf. ACM. 2005, pp. 1539–1546.
 """
    def __init__(self, n_ops, adaptation_rate = 0.6):
        super().__init__(n_ops)
        self.adaptation_rate = adaptation_rate
        debug_print("\n {} : adaptation_rate = {}".format(type(self).__name__, self.adaptation_rate))
    
    def calc_quality(self, old_reward, reward, old_probability):
        quality = self.old_quality + self.adaptation_rate * (reward - self.old_quality)
        # Q = Q - np.max(Q)
        return super().check_quality(quality)

class Upper_confidence_bound(QualityType):
    """
Alvaro Fialho et al. “Extreme value based adaptive operator selection”.In:International Conference on Parallel Problem Solving from Nature.https : / / hal . inria . fr / file / index / docid / 287355 / filename /rewardPPSN.pdf. Springer. 2008, pp. 175–184
"""
    def __init__(self, n_ops, off_met, window, scaling_factor = 0.5):
        super().__init__(n_ops)
        self.off_met = off_met
        # MANUEL: There is a window here, but which object handles it?
        # MANUEL: This window is not the same object as the one in AOS nor the one in RewardType!
        self.window = window
        self.scaling_factor = scaling_factor
        debug_print("\n {} : scaling_factor = {}".format(type(self).__name__, self.scaling_factor))
    
    def calc_quality(self, old_reward, reward, old_probability):
        window_op_sorted, N, rank = count_op(self.n_ops, self.window, self.off_met)
        quality = UCB(N, self.scaling_factor, reward); # print(window_op_sorted, N, rank, reward, Q)
        return super().check_quality(quality)

class Identity(QualityType):
    def __init__(self, n_ops):
        super().__init__(n_ops)
    
    def calc_quality(self, old_reward, reward, old_probability):
        quality = reward
        return super().check_quality(quality)

class Weighted_normalised_reward(QualityType):
    """
Christian  Igel  and  Martin  Kreutz.  “Operator  adaptation  in  evolution-ary  computation  and  its  application  to  structure  optimization  of  neu-ral  networks”.  In:Neurocomputing55.1-2  (2003).https : / / ac . els -cdn.com/S0925231202006288/1-s2.0-S0925231202006288-main.pdf?_tid=c6274e78-02dc-4bf6-8d92-573ce0bed4c4&acdnat=1540907096_d0cc1e2b4ca56a49587b4d55e1008a84, pp. 347–361
"""
    def __init__(self, n_ops, decay_rate = 0.3):
        super().__init__(n_ops)
        self.decay_rate = decay_rate
        debug_print("\n {} : decay_rate = {}".format(type(self).__name__, self.decay_rate))
    
    def calc_quality(self, old_reward, reward, old_probability):
        if np.sum(reward) > 0:
            reward /= np.sum(reward)
        else:
            reward = self.n_ops
        quality = self.decay_rate * reward  + (1 - self.decay_rate) * self.old_quality
        # if np.sum(reward) > 0:
        #     quality = self.decay_rate * (reward / np.sum(reward))  + (1 - self.decay_rate) * self.old_quality
        # else:
        #     quality = self.decay_rate * self.n_ops + (1 - self.decay_rate) * self.old_quality
        return super().check_quality(quality)

class Markov_reward_process(QualityType):
    """
 Mudita  Sharma,  Manuel  L ́opez-Ib ́a ̃nez,  and  Dimitar  Kazakov.  “Perfor-mance Assessment of Recursive Probability Matching for Adaptive Oper-ator Selection in Differential Evolution”. In:International Conference onParallel Problem Solving from Nature.http://eprints.whiterose.ac.uk/135483/1/paper_66_1_.pdf. Springer. 2018, pp. 321–333.
 """
    def __init__(self, n_ops, memory_curr_reward = 1, memory_prev_reward = 0.9, discount_rate = 0.0):
        super().__init__(n_ops)
        self.memory_curr_reward = memory_curr_reward
        self.memory_prev_reward = memory_prev_reward
        self.discount_rate = discount_rate
        #tran_matrix = normalize_matrix(np.random.rand(self.n_ops, self.n_ops))
        debug_print("\n {} : memory_curr_reward = {}, memory_prev_reward = {}, discount_rate = {}".format(type(self).__name__, self.memory_curr_reward, self.memory_prev_reward, self.discount_rate))
    
    def calc_quality(self, old_reward, reward, old_probability):
        tran_matrix = TM(self.n_ops, old_probability)
        quality = self.memory_curr_reward * reward + self.memory_prev_reward * old_reward
        # MANUEL: Where does this formula come from?
        # MUDITA: This is from our PPSN paper.
        quality = np.matmul(np.linalg.pinv(np.array(1 - self.discount_rate * tran_matrix)), np.array(Q))
        # MANUEL: you overwrite the line above?
        # MUDITA: Which line?
        quality = quality - np.max(quality)
        # MANUEL: you overwrite the line above?
        # MUDITA: Which line?
        quality = np.exp(quality)
        return super().check_quality(quality)



#################################################Probability definitions######################################################################

def build_probability(choice, n_ops, prob_args):
    if choice == 0:
        return Probability_Matching(n_ops, prob_args["p_min"], prob_args["error_prob"])
    elif choice == 1:
        return Adaptive_Pursuit(n_ops, prob_args["p_min"], prob_args["p_max"], prob_args["learning_rate"])
    elif choice == 2:
        return Adaptation_rule(n_ops, prob_args["p_min"], prob_args["learning_rate"])
    else:
        raise ValueError("choice {} unknown".format(choice))
 
class ProbabilityType(ABC):
    # Static variables
    # FIXME: Use __slots__ to find which parameters need to be defined.
    args_names = ["p_min", "learning_rate", "error_prob", "p_max"]
    # FIXME: define this in the class as @property getter doctstring and get it from it
    args_help = ["Minimum probability of selection of an operator", "Learning Rate", "Probability noise", "Maximum probability of selection of an operator"]
    def __init__(self, n_ops, p_min = None, learning_rate = None):
        # n_ops, p_min_prob and beta_prob used in more than one probability definition
        self.p_min = p_min
        self.learning_rate = learning_rate
        self.old_probability = np.full(n_ops, 1.0 / n_ops)
        self.eps = np.finfo(self.old_probability.dtype).eps

    @classmethod
    def add_argument(cls, parser):
        "Add arguments to an ArgumentParser"
        for arg, help in zip(cls.args_names, cls.args_help):
            parser.add_argument('--' + arg, type=float, default=0, help=help)
        return cls.args_names

    def check_probability(self, probability):
        probability /= np.sum(probability)
        assert np.allclose(np.sum(probability), 1.0, equal_nan = True)
        assert np.all(probability >= -0.0)
        # MANUEL: This is wrong! It creates a view of an array and not a copy
        # self.old_probability = self.probability
        self.old_probability = probability
        return(probability)

    @abstractmethod
    def calc_probability(self, quality):
        "Must be implemented by derived probability methods"
        pass
    
# MANUEL: These should have more descriptive names and a doctstring documenting
# where they come from (references) and what they do.
class Probability_Matching(ProbabilityType):
    """
 Dirk Thierens. “An adaptive pursuit strategy for allocating operator prob-abilities”.  In:Proceedings of the 7th annual conference on Genetic andevolutionary computation.http://www.cs.bham.ac.uk/~wbl/biblio/gecco2005/docs/p1539.pdf. ACM. 2005, pp. 1539–1546.
"""
    def __init__(self, n_ops, p_min = 0.1, error_prob = 0.0):
        super().__init__(n_ops, p_min = p_min)
        # np.finfo(np.float32).eps adds a small epsilon number that doesn't make any difference but avoids 0.
        self.error_prob = error_prob + self.eps
        # MANUEL: Please do this in every class so one can debug what is actually running.
        debug_print("\n {} : p_min = {}, error_prob = {}".format(type(self).__name__, self.p_min, self.error_prob))
        
    def calc_probability(self, quality):
        probability = self.p_min + (1 - len(quality) * self.p_min) * ((quality + self.error_prob) / np.sum(quality + self.error_prob))
        return super().check_probability(probability)
        

class Adaptive_Pursuit(ProbabilityType):
    """ Proposed by:

Dirk Thierens. “An adaptive pursuit strategy for allocating operator prob-
abilities”. In: Proceedings of the 7th annual conference on Genetic and
evolutionary computation. http://www.cs.bham.ac.uk/~wbl/biblio/gecco2005/docs/p1539.pdf. ACM. 2005, pp. 1539–1546.

"""
    def __init__(self, n_ops, p_min = 0.1, p_max = 0.9, learning_rate = 0.1):
        super().__init__(n_ops, p_min = p_min, learning_rate = learning_rate)
        self.p_max = p_max
        debug_print("\n {} : p_min = {}, p_max = {}, learning_rate = {}".format(type(self).__name__, self.p_min, self.p_max, self.learning_rate))

    def calc_probability(self, quality):
        delta = np.full(quality.shape[0], self.p_min)
        delta[np.argmax(quality)] = self.p_max
        probability = self.old_probability + self.learning_rate  * (delta - self.old_probability)
        probability += self.eps
        return super().check_probability(probability)

class Adaptation_rule(ProbabilityType):
    """
Christian Igel and Martin Kreutz. “Using fitness distributions to improvethe evolution of learning structures”. In:Evolutionary Computation, 1999.CEC 99. Proceedings of the 1999 Congress on. Vol. 3.http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.43.2107&rep=rep1&type=pdf. IEEE. 1999, pp. 1902–1909
"""
    def __init__(self, n_ops, p_min = 0.025, learning_rate = 0.5):
        super().__init__(n_ops, p_min = p_min, learning_rate = learning_rate)
        debug_print("\n {} : p_min = {}, learning_rate = {}".format(type(self).__name__, self.p_min, self.learning_rate))
        
    def calc_probability(self, quality):
        # MANUEL: Why do this?
        if np.sum(quality) != 0:
            quality = quality.copy()
            # Normalize
            quality += self.eps
            quality /= np.sum(quality)

        # np.maximum is element-wise
        probability = self.learning_rate * np.maximum(self.p_min, quality) + (1.0 - self.learning_rate) * self.old_probability
        probability += self.eps
        return super().check_probability(probability)


##################################################Selection definitions######################################################################

def build_selection(choice, n_ops):
    if choice == 0:
        return Propotional_Selection(n_ops)
    elif choice == 1:
        return Greedy_Selection(n_ops)
    else:
        raise ValueError("choice {} unknown".format(choice))

class SelectionType(ABC):
    def __init__(self, n_ops):
        # The initial list of operators (randomly permuted)
        self.n_ops = n_ops
        self.op_init_list = list(np.random.permutation(n_ops))
    
    def check_selection(self, selected):
        assert selected >= 0 and selected <= self.n_ops
        return selected
    
    @abstractmethod
    def perform_selection(self, probability):
        pass

# MANUEL: These should have more descriptive names and a doctstring documenting
# where they come from (references) and what they do.
class Propotional_Selection(SelectionType):
    def __init__(self, n_ops):
        super().__init__(n_ops)
    
    def perform_selection(self, probability):
        # Roulette wheel selection
        if self.op_init_list:
            SI = self.op_init_list.pop()
        else:
            SI = np.random.choice(len(probability), p = probability)
        return super().check_selection(SI)


class Greedy_Selection(SelectionType):
    def __init__(self, n_ops):
        super().__init__(n_ops)
    
    def perform_selection(self, probability):
        # Greedy Selection
        if self.op_init_list:
            SI = self.op_init_list.pop()
        else:
            SI = np.argmax(probability)
        return super().check_selection(SI)


# # Parameters: p_min_prob0, e_prob0
# def Probability0(n_ops, quality, p_min_prob0, e_prob0, p_min_prob1, p_max_prob1, beta_prob1, p_min_prob2, beta_prob2, old_probability):
#     probability = p_min_prob + (1 - len(quality) * p_min_prob0) * ((quality + e_prob0 + np.finfo(np.float32).eps) / (np.sum(quality + e_prob0 + np.finfo(np.float32).eps)))
#     probability = probability/np.sum(probability)
#     return(probability)

# Parameters: beta_prob1, p_min_prob1, p_max_prob1
# def Probability1(n_ops, quality, p_min_prob0, e_prob0, p_min_prob1, p_max_prob1, beta_prob1, p_min_prob2, beta_prob2, old_probability):
#     probability = np.zeros(n_ops)
#     probability = old_probability + beta_prob1 * (p_min_prob1 - old_probability)
#     probability[np.argmax(quality)] = old_probability[np.argmax(quality)] + beta_prob1 * (p_max_prob1 - old_probability[np.argmax(quality)])
#     return ((probability + np.finfo(np.float32).eps) / (np.sum(probability + np.finfo(np.float32).eps)))

# # Parameters: beta_prob2, p_min_prob2
# def Probability2(n_ops, quality, p_min_prob0, e_prob0, p_min_prob1, p_max_prob1, beta_prob1, p_min_prob2, beta_prob2, old_probability):
#     probability = np.zeros(n_ops)
#     #print("qual: ",quality)
#     for i in range(n_ops): # Direct way??????
#         if np.sum(quality) != 0:
#             probability[i] = beta_prob2 * np.max([p_min_prob2, np.array(quality[i] + np.finfo(np.float32).eps) / (np.sum(quality + np.finfo(np.float32).eps))]) + (1 - beta_prob2) * np.array(old_probability[i])
#         else:
#             probability[i] = beta_prob2 * np.max([p_min_prob2, np.array(quality[i])]) + (1 - beta_prob2) * np.array(old_probability[i])
#     #print(probability / np.sum(probability))
#     return ((probability + np.finfo(np.float32).eps) / (np.sum(probability + np.finfo(np.float32).eps)))

# def Probability3(n_ops, quality, p_min_prob0, e_prob0, p_min_prob1, p_max_prob1, beta_prob1, p_min_prob2, beta_prob2, old_probability):
#     probability = np.zeros(n_ops)
#     Probability = quality
#     return ((probability + np.finfo(np.float32).eps) / (np.sum(probability + np.finfo(np.float32).eps)))
    
                              
##################################################Selection definitions######################################################################

#def Selection0(op_init_list, p):
    # Roulette wheel selection
#    if op_init_list:
#        SI = op_init_list.pop()
#    else:
#        SI = np.random.choice(len(p), p = p)
#    return SI


#def Selection1(op_init_list, p):
    # Greedy Selection
#    if op_init_list:
#        SI = op_init_list.pop()
#    else:
#        SI = np.argmax(p)
#    return SI


##################################################Unknown_AOS######################################################################

class Unknown_AOS(AOS_Update):
    def __init__(self, popsize, F1, F, u, X, f_min, x_min, best_so_far, best_so_far1, n_ops, OM_choice, rew_choice, rew_args, qual_choice, qual_args, prob_choice, prob_args, select_choice):
        super(Unknown_AOS,self).__init__(popsize, F1, F, u, X, f_min, x_min, best_so_far, best_so_far1, n_ops)
        self.reward = np.zeros(self.n_ops)
        self.old_reward = self.reward.copy()
        self.reward_type = build_reward(rew_choice, n_ops, rew_args, self.gen_window, self.window, OM_choice, popsize)
        self.quality = np.full(n_ops, 1.0)
        self.old_quality = self.quality.copy()
        self.quality_type = build_quality(qual_choice, n_ops, qual_args, self.window, OM_choice)
        # self.probability = np.zeros(self.n_ops); self.probability[:] = 1.0 / len(self.probability)
        self.probability = np.full(n_ops, 1.0 / n_ops)
        self.probability_type = build_probability(prob_choice, n_ops, prob_args)
        self.selection_type = build_selection(select_choice, n_ops)


